# configs/config.yaml
# Configuration for Biosignal Foundation Model Training


downsample:
  segment_length_sec: 10
# Dataset settings
dataset:
  name: "BUT_PPG"
  data_dir: "data/but_ppg/dataset"
  quality_file: "quality-hr-ann.csv"
  subject_file: "subject-info.csv"

  # Signal parameters (following Apple paper)
  ppg:
    original_fs: 30      # Hz (actual from BUT PPG)
    target_fs: 64        # Hz (can keep for consistency)
    segment_length: 30   # seconds (actual data length)
    band_low: 0.4        # Hz
    band_high: 8.0       # Hz

  ecg:
    original_fs: 100     # Hz (actual from BUT PPG)
    target_fs: 128       # Hz (can keep for consistency)
    segment_length: 30   # seconds (actual data length)
    band_low: 0.5        # Hz
    band_high: 40.0      # Hz

  acc:
    original_fs: 100     # Hz (actual from BUT PPG)
    target_fs: 100       # Hz (keep original)
    segment_length: 10   # seconds (actual data length)
    band_low: 0.1        # Hz
    band_high: 20.0      # Hz
    channels: 3

# Model architecture (Apple paper specifications)
model:
  name: "EfficientNet1D"
  embedding_dim: 256
  projection_dim: 128
  n_blocks: 16           # Paper specifies 16 MBConv blocks
  width_multiplier: 1.0
  depth_multiplier: 1.0
  dropout_rate: 0.1
  drop_path_rate: 0.1

simsiam:
  # Architecture parameters
  projection_dim: 2048   # SimSiam paper default
  prediction_dim: 512    # SimSiam paper default

  # Training parameters optimized for small datasets
  batch_size: 32         # Can work with smaller batches than InfoNCE
  learning_rate: 0.00005 # Higher than InfoNCE since no momentum
  weight_decay: 0.0001   # Less regularization than InfoNCE

  # No temperature or momentum needed for SimSiam
  # No negative pairs needed

  # Augmentations can be stronger since no negatives
  augmentations_ppg:
    cutout: 0.8         # Stronger than InfoNCE
    magnitude_warp: 0.6
    gaussian_noise: 0.6
    time_warp: 0.5
    channel_permute: 0.0

  augmentations_ecg:
    cutout: 0.95        # Even stronger augmentation
    magnitude_warp: 0.8
    gaussian_noise: 0.8
    time_warp: 0.5
    channel_permute: 0.0

  augmentations_acc:
    cutout: 0.4
    magnitude_warp: 0.3
    gaussian_noise: 0.3
    time_warp: 0.2
    channel_permute: 0.5  # Important for ACC rotation invariance


# SSL training parameters (Apple paper)
ssl:
  # Loss function
  temperature: 0.07    # InfoNCE temperature
  lambda_koleo: 0.05      # KoLeo regularization weight
  momentum_rate: 0.99    # Momentum encoder update rate

  # Augmentations (paper Table 1)
  augmentations_ppg:
    cutout: 0.7
    magnitude_warp: 0.5
    gaussian_noise: 0.5
    time_warp: 0.3
    channel_permute: 0.0  # Single channel for PPG

  augmentations_ecg:
    cutout: 0.9
    magnitude_warp: 0.8
    gaussian_noise: 0.8
    time_warp: 0.6
    channel_permute: 0.0  # Single channel for ECG

  augmentations_acc:
    cutout: 0.6
    magnitude_warp: 0.4
    gaussian_noise: 0.4
    time_warp: 0.2
    channel_permute: 0.25  # Important for 3-axis ACC (rotation invariance)

# Training parameters
training:
  # Batch and epochs
  batch_size: 64         # Smaller for M1 (paper uses 256)
  num_epochs: 30        # Paper uses 300, we'll use less for testing

  # Optimizer
  optimizer: "adam"
  learning_rate: 0.0001
  weight_decay: 0.00001

  # Learning rate schedule
  scheduler: "step"
  lr_step_size: 30
  lr_gamma: 0.1

  # Data loading
  num_workers: 8         # For M1 chip
  pin_memory: false      # Not needed for MPS

  # Checkpointing
  save_freq: 30
  checkpoint_dir: "data/outputs/checkpoints"

  # Logging
  log_freq: 10
  use_wandb: false
  wandb_project: "biosignal-foundation"

  # Mixed precision (for faster training)
  use_amp: false         # Set to true if using CUDA
  gradient_accumulation_steps: 1  # Increase for larger effective batch size

# Evaluation parameters
evaluation:
  # Downstream tasks
  tasks:
    - "age_classification"    # >50 vs ≤50
    - "age_regression"
    - "sex_classification"    # M vs F
    - "bmi_classification"     # >30 vs ≤30
    - "bmi_regression"
    - "bp_classification"      # Hypertensive vs normal
    - "spo2_prediction"
    - "hr_estimation"
    - "activity_classification"  # For ACC: activity type classification
    - "step_counting"            # For ACC: step count estimation
    - "fall_detection"           # For ACC: fall detection

  # Linear probe
  ridge_alpha: 1.0
  cv_folds: 5

  # Metrics
  metrics:
    classification: ["auc", "pauc_0.1", "accuracy", "f1"]
    regression: ["mae", "rmse", "r2"]

# Device settings
device:
  backend: "mps"         # For M1 chip (mps, cuda, or cpu)
  mixed_precision: false # MPS doesn't fully support amp yet

# Reproducibility
seed: 42

# Paper benchmark results (for comparison)
paper_benchmarks:
  ppg:
    age_classification_auc: 0.976
    sex_classification_auc: 0.993
    bmi_classification_auc: 0.918
    age_regression_mae: 3.19
    bmi_regression_mae: 2.54

  ecg:
    age_classification_auc: 0.916
    sex_classification_auc: 0.951
    bmi_classification_auc: 0.797
    age_regression_mae: 6.33
    bmi_regression_mae: 3.72

  acc:
    # ACC benchmarks would be different - these are placeholders
    # Real benchmarks would come from your experiments
    activity_classification_auc: 0.90  # Example
    step_counting_mae: 5.0             # Example
    fall_detection_auc: 0.95           # Example