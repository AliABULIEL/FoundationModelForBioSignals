# configs/config.yaml
# Configuration for Biosignal Foundation Model Training


downsample:
  enabled: false
  segment_length_sec: 10
# Dataset settings

  # Optional preprocessed data directory
  preprocessed_dir: null  # Set to path if using preprocessed data
dataset:
  # General dataset settings
  name: "BUT_PPG"
  data_dir: "data/but_ppg/dataset"
  subject_file: "subject-info.csv"
  quality_file: "quality-hr-ann.csv"
  train_ratio: 0.5
  val_ratio: 0.2
  use_cache: true
  cache_size: 500

  # NEW: Pair generation settings for BUT PPG
  pairs_per_participant: 20  # Target pairs per participant
  max_pairs_per_participant: 100  # Hard limit to prevent explosion
  min_recordings_for_pairs: 2  # Minimum recordings needed

  # Data split ratios

  # test_ratio is computed as 1 - train_ratio - val_ratio

  # Segment overlap for data augmentation
  segment_overlap: 0.5

  # Signal parameters (following Apple paper)
  ppg:
    original_fs: 30      # Hz (actual from BUT PPG)
    target_fs: 64        # Hz (can keep for consistency)
    segment_length: 10   # seconds (actual data length)
    band_low: 0.4        # Hz
    band_high: 8.0       # Hz

  ecg:
    original_fs: 100     # Hz (actual from BUT PPG)
    target_fs: 128       # Hz (can keep for consistency)
    segment_length: 10   # seconds (actual data length)
    band_low: 0.5        # Hz
    band_high: 40.0      # Hz

  acc:
    original_fs: 100     # Hz (actual from BUT PPG)
    target_fs: 100       # Hz (keep original)
    segment_length: 10   # seconds (actual data length)
    band_low: 0.1        # Hz
    band_high: 20.0      # Hz
    channels: 3

# Model architecture (Apple paper specifications)
model:
  name: "EfficientNet1D"
  embedding_dim: 256
  projection_dim: 128
  n_blocks: 16           # Paper specifies 16 MBConv blocks
  width_multiplier: 0.5
  depth_multiplier: 1.0
  dropout_rate: 0.1
  drop_path_rate: 0.1

simsiam:
  # Architecture parameters
  projection_dim: 2048   # SimSiam paper default
  prediction_dim: 512    # SimSiam paper default

  # Training parameters optimized for small datasets
  batch_size: 32         # Can work with smaller batches than InfoNCE
  learning_rate: 0.00005 # Higher than InfoNCE since no momentum
  weight_decay: 0.0001   # Less regularization than InfoNCE

  # No temperature or momentum needed for SimSiam
  # No negative pairs needed

  # Augmentations can be stronger since no negatives
  augmentations_ppg:
    cutout: 0.8         # Stronger than InfoNCE
    magnitude_warp: 0.6
    gaussian_noise: 0.6
    time_warp: 0.5
    channel_permute: 0.0

  augmentations_ecg:
    cutout: 0.2        # Even stronger augmentation
    magnitude_warp: 0.3
    gaussian_noise: 0.3
    time_warp: 0.2
    channel_permute: 0.0

  augmentations_acc:
    cutout: 0.4
    magnitude_warp: 0.3
    gaussian_noise: 0.3
    time_warp: 0.2
    channel_permute: 0.5  # Important for ACC rotation invariance


# SSL training parameters (Apple paper)
ssl:
  temperature: 0.07  # Match paper
  lambda_koleo: 0.1  # Match paper
  momentum_rate: 0.99  # Match paper

  # Augmentations (paper Table 1)
  augmentations_ppg:
    cutout: 0.4  # Paper value
    magnitude_warp: 0.25
    gaussian_noise: 0.25
    time_warp: 0.15
    channel_permute: 0.0

  augmentations_ecg:
    cutout: 0.8  # Paper value
    magnitude_warp: 0.5
    gaussian_noise: 0.5
    time_warp: 0.3
    channel_permute: 0.0

  augmentations_acc:
    cutout: 0.6
    magnitude_warp: 0.4
    gaussian_noise: 0.4
    time_warp: 0.2
    channel_permute: 0.25  # Important for 3-axis ACC (rotation invariance)

# Training parameters
training:
  # Batch and epochs
  batch_size: 64         # Smaller for M1 (paper uses 256)
  num_epochs: 30        # Paper uses 300, we'll use less for testing

  # Optimizer
  optimizer: "adam"
  learning_rate: 0.0001
  weight_decay: 0.00001

  # Learning rate schedule
  scheduler: "cosine"
  lr_step_size: 30
  lr_gamma: 0.1
  early_stopping_patience: 10

  # Data loading
  num_workers: 8         # For M1 chip
  pin_memory: false      # Not needed for MPS

  # Checkpointing
  save_freq: 30
  checkpoint_dir: "data/outputs/checkpoints"

  # Logging
  log_freq: 10
  use_wandb: false
  wandb_project: "biosignal-foundation"

  # Mixed precision (for faster training)
  use_amp: false         # Set to true if using CUDA
  gradient_accumulation_steps: 1  # Increase for larger effective batch size

# Evaluation parameters
evaluation:
  # Downstream tasks
  tasks:
    - "age_classification"    # >50 vs ≤50
    - "age_regression"
    - "sex_classification"    # M vs F
    - "bmi_classification"     # >30 vs ≤30
    - "bmi_regression"
#    - "bp_classification"      # Hypertensive vs normal
    - "spo2_prediction"
    - "hr_estimation"
    - "activity_classification"  # For ACC: activity type classification
    - "step_counting"            # For ACC: step count estimation
    - "fall_detection"           # For ACC: fall detection

  # Linear probe
  ridge_alpha: 1.0
  cv_folds: 5
  batch_size: 512
  num_workers: 4



  # Metrics
  metrics:
    classification: ["auc", "pauc_0.1", "accuracy", "f1"]
    regression: ["mae", "rmse", "r2"]

# Device settings
device:
  backend: "mps"         # For M1 chip (mps, cuda, or cpu)
  mixed_precision: false # MPS doesn't fully support amp yet

# Reproducibility
seed: 42

# Paper benchmark results (for comparison)
paper_benchmarks:
  ppg:
    age_classification_auc: 0.976
    sex_classification_auc: 0.993
    bmi_classification_auc: 0.918
    age_regression_mae: 3.19
    bmi_regression_mae: 2.54

  ecg:
    age_classification_auc: 0.916
    sex_classification_auc: 0.951
    bmi_classification_auc: 0.797
    age_regression_mae: 6.33
    bmi_regression_mae: 3.72

  acc:
    # ACC benchmarks would be different - these are placeholders
    # Real benchmarks would come from your experiments
    activity_classification_auc: 0.90  # Example
    step_counting_mae: 5.0             # Example
    fall_detection_auc: 0.95           # Example



# Add these sections to your existing config.yaml

# VitalDB settings
vitaldb:
  cache_dir: "data/vitaldb_cache"
  train_ratio: 0.8
  val_ratio: 0.1
  max_pairs_per_case: 10
  cases_limit: 6000  # Limit number of cases to use (optional)
  track_mapping:
    ppg: "PLETH"
    ecg: "ECG_II"
    acc: "ACC"
  segments_per_case: 15
  sampling_rates:
    ppg: 100
    ecg: 500
    acc: 100

# Pre-training settings
pretrain:
  dataset: "vitaldb"
  epochs: 50
  batch_size: 128
  learning_rate: 0.0001
  num_workers: 8
  save_freq: 10
  early_stopping_patience: 15

# Fine-tuning settings
finetune:
  dataset: "but_ppg"
  epochs: 20
  batch_size: 64
  learning_rate: 0.00001
  num_workers: 4
  freeze_encoder: false
  save_freq: 5
  early_stopping_patience: 10

